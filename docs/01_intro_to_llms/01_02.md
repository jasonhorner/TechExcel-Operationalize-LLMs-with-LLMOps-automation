---
title: '2. Work with an Open Source LLM Model'
layout: default
nav_order: 3
parent: 'Exercise 01: Introduction to LLMs and Azure AI Services'
---

# Task 02 - Work with an Open Source LLM Model

## Introduction

Lamna Healthcare is interested in evaluating multiple models to determine which one best suits their needs. They require a solution that enables them to manually test multiple models quickly.

## Description

In this task, you will deploy an open-source model from the Azure AI Studio model catalog. You will repeat the conversation transcript JSON data extraction, summarization and sentiment analysis that you performed in the previous task. Only this time, you will use the `Llama-2-13b-chat` model to generate the JSON document instead of `gpt-4`.

The key tasks are as follows:

1. Using [Azure AI Studio](https://ai.azure.com), deploy the `Llama-2-13b-chat` model from the catalog using Managed Compute.

> [!Note]
> if you run into a quota limitation you can deploy a less resource instensive model such as Phi-3

2. Test the model with a sample input payload based on your system message you defined in the previous task.

**Hint**: By clicking the model name link below the **Model ID** field on the model deployment page, you can access the model documentation and see the input and output schema.
if you run into a quaota issue you can deploy a less resource instensive model such as Phi-3


## Success Criteria

* Consistently results in the LLM returning accurate and properly formatted JSON based on the provided conversation transcript using an open-source model.

## Solution

<details markdown="block">
<summary>Expand this section to view the solution</summary>

##### 1) Work with an Open Source LLM Model

In this step, you'll deploy an open source Llama 2 model from Meta.

1. In [Azure AI Studio](https://ai.azure.com), ensure you are in the project you created in the previous task, and select **Deployments** from the left-hand menu.

2. Select **+ Create deployment**.
  
   ![The model deployments list displays. The + Create deployment button is visible.](images/labgrab17.png)

3. Search for and select the model **Llama-2-13b-chat** then select **Confirm**.

   ![The model catalog displays with the Llama-2-13b-chat model selected.](images/labgrab18.png)

4. Select the deployment option **Managed Compute without Azure AI Content Safety**.
   ![The deployment options dialog displays with choices of Serverless API with Azure AI Content Safety or Managed Compute without Azure AI Content Safety.](images/labgrab19.png)

5. Check the box that shows **I want to use shared quota and I acknowledge that this endpoint will be deleted in 168 hours.**.

6. Select the **Standard_NC24s_v3** compute for inference with the selected model, for this workshop one instance is enough.

7. If you do not have enough quota you can access the AzureML Quota option in the Managed tab to request an increase in quota for the selected resource, or choose a different compute with quota availability.

8. Click the **Deploy** button.

   ![The Deploy model dialog displays with the Virtual Machine compute selection set to Standard_NC24s_v3.](images/labgrab20.png)

9. The creation of the deployment will take a few minutes, the time varies, but generally between 10 and 20 minutes. (If you notice your Traffic allocation is 0% select **Update traffic** and set it to 100%, if you get an error try again.)

   ![The model deployment screen displays with the Provisioning state indicating Succeeded.](images/labgrab21.png)

10. Using a text editor, create a file named `llama2_input.json` and paste the following JSON payload into the file. Save this file in a known location, we'll be using it in the next step. Notice how the system message in in the input data, as well as the user prompt:

    ```json
    {
    "input_data": {
      "input_string": [
        {
          "role": "system",
          "content": "You're an AI assistant that helps Lamna Healthcare Customer Service to extract valuable information from their conversations by creating JSON documents for each conversation transcription you receive. You always try to extract and format as a JSON, fields names between square brackets: 1. Customer Name [name] 2. Customer Contact Phone [phone] 3. Main Topic of the Conversation [topic] 4. Customer Sentiment (Neutral, Positive, Negative)[sentiment] 5. How the Agent Handled the Conversation [agent_behavior] 6. What was the FINAL Outcome of the Conversation [outcome] 7. A really brief Summary of the Conversation [summary] Only extract information that you're sure. If you're unsure, write 'Unknown/Not Found' in the JSON file. Your answers outputs contains only the json document."
        },
        {
          "role": "user",
          "content": "Agent: Hello, welcome to Lamna Healthcare customer service. My name is Juan, how can I assist you? Client: Hello, Juan. I'm calling because I'm having issues with my medical bill I just received few days ago. It's incorrect and it does not match the numbers I was presented before my medical procedure. Agent: I'm very sorry for the inconvenience, sir. Could you please tell me your phone number and your full name? Client: Yes, sure. My number is 011-4567-8910 and my name is Martín Pérez. Agent: Thank you, Mr. Pérez. I'm going to check your plan, you deduction limits and current year transactions towards your deductions. One moment, please. Client: Okay, thank you. Agent: Mr. Pérez, I've reviewed your plan and I see that you have the Silver basic plan of $3,000 deductable. Is that correct? Client: Yes, that's correct. Agent: Well, I would like to inform you that you have not met your deductible yet and $2,800 of the procedure will be still be your responsability and that will meet your deductible for the year. Client: What? How is that possible? I paid over $2,000 already towards my deductable this year, I should only be $1,000 away from reaching my deductible not $2,800.  Agent: I understand, Mr. Pérez. But keep in mind that not all fees your pay to doctors and labs and medications count towards your deductible.  Client: Well, but they didn't explain that to me when I contracted the plan. They told me that everything I pay from my pocket towards doctors, specialists, labs and medications will count towards my deductable. I feel cheated. Agent: I apologize, Mr. Pérez. It was not our intention to deceive you. If you think the deductable is too high, I recommed changing the plan to Gold at the next renewal window and that will bring the deductable to $1,000 for the new year. Client: And how much would that cost me? Agent: The plan rates will come out in November, you can call us back then or check the new rates online at that time. Client: Mmm, I don't know. Isn't there another option? Can't you reduce the amount I have to pay for this bill as I was not explained how the deductible work correctly? Agent: I'm sorry, Mr. Pérez. I don't have the power to change the bill or your deductible under the current Silver plan. Client: Well, let me think about it. Can I call later to confirm? Agent: Of course, Mr. Pérez. You can call whenever you want. The number is the same one you dialed now. Is there anything else I can help you with? Client: No, that's all. Thank you for your attention. Agent: Thank you, Mr. Pérez. Have a good day. Goodbye."
        }
      ],
      "parameters": {
        "temperature": 0.8,
        "top_p": 0.8,
        "do_sample": true,
        "max_new_tokens": 1000
      }
    }
  }
    ```

11. Verify the deployment has succeeded (the Provisioning state will display **Succeeded**). Keep the deployment page open. Open notepad or another text editor to beging building a `curl` command to test the model. Replace `<endpoint>` with the endpoint URL of your deployment, and `<apikey>` with the **Primary key** value of your deployment. Run the command in a terminal window command prompt in the same directory as the `llama2_input.json` file you created in the previous step.

    ![The model deployment page displays with the Succeeded Provisioning State highlighted. The endpoint and primary key fields are also highlighted.](images/labgrab22.png)

    ```bash
    curl -X POST "<endpoint>" -H "Content-Type: application/json" -H "Authorization: Bearer <apikey>" -d @llama2_input.json
    ```

12. Observe the response generated by the Llama2 model. You should see a result generated by the model in the command window.

13. Optionally, select the **Details** tab of the deployed model and choose **Open in playground** to test the model using the Azure AI Studio Playground experience. Remember to set the system message and send in the transcript in the chat just as you did in the previous task with `gpt-4` (not using JSON formatting). Similarly, using the **Test** tab on the model deployment page also provides a chat user interface to interact with the deployed model.

</details>
